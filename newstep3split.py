#!/usr/bin/env python
# step3split.py  â€“Â quickâ€‘test version (10â€¯% data, 1 epoch by default)

import argparse, os, pickle, numpy as np, matplotlib.pyplot as plt
from clearml import Task, Dataset
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2, DenseNet121
from tensorflow.keras import layers, models, Input, optimizers
from tensorflow.keras.callbacks import ModelCheckpoint
import seaborn as sns

class_names = None
num_classes = None

# â”€â”€â”€ 1.Â Tunables & quickâ€‘test flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser()
parser.add_argument('--lr',            type=float, default=1e-3,  help='Learning rate')
parser.add_argument('--batch_size',    type=int,   default=32,    help='Batch size')
parser.add_argument('--dropout',       type=float, default=0.4,   help='Dropâ€‘out rate')
parser.add_argument('--epochs',        type=int,   default=1,     help='Training epochs')
parser.add_argument('--subset_ratio',  type=float, default=0.10,  help='Fraction of data to use (0â€‘1)')
args = parser.parse_args()

# â”€â”€â”€ 2.Â Register with ClearML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
task = Task.init(project_name="VisiblePipeline", task_name="step_train")
task.connect(args)          # lr, batch_size, dropout, epochs, subset_ratio
logger = task.get_logger()

# â”€â”€â”€ 3.Â Dataset paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dataset = Dataset.get(
    dataset_name="plant_processed_data_split",
    dataset_project="VisiblePipeline",
    only_completed=True
)
root = dataset.get_local_copy()
dirs = {k: os.path.join(root, k) for k in ['train', 'valid', 'test']}

# ðŸ”„Â REPLACE the make_ds helper with this version
def make_ds(split):
    full_ds = tf.keras.preprocessing.image_dataset_from_directory(
        dirs[split],
        image_size=(160, 160),
        batch_size=args.batch_size,
        shuffle=(split == 'train')
    )
    # save class names only once (theyâ€™re identical for every split)
    global class_names, num_classes
    if class_names is None:
        class_names = full_ds.class_names
        num_classes = len(class_names)

    # downâ€‘sample if requested
    if 0 < args.subset_ratio < 1.0:
        n = tf.data.experimental.cardinality(full_ds).numpy()
        full_ds = full_ds.take(int(n * args.subset_ratio))

    return full_ds.prefetch(tf.data.AUTOTUNE)

train_ds = make_ds('train')
val_ds   = make_ds('valid')
test_ds  = make_ds('test')
class_names = train_ds.class_names
num_classes = len(class_names)

# â”€â”€â”€ 4.Â Build hybrid model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
inp       = Input(shape=(160, 160, 3))
mnet      = MobileNetV2(include_top=False, weights='imagenet', input_tensor=inp)
densenet  = DenseNet121(include_top=False, weights='imagenet', input_tensor=inp)
mnet.trainable = True
densenet.trainable = False

out = layers.Concatenate()([
        layers.GlobalAveragePooling2D()(mnet.output),
        layers.GlobalAveragePooling2D()(densenet.output)
      ])
out = layers.Dense(256, activation='relu')(out)
out = layers.BatchNormalization()(out)
out = layers.Dropout(args.dropout)(out)
out = layers.Dense(num_classes, activation='softmax')(out)
model = models.Model(inputs=inp, outputs=out)

model.compile(
    optimizer=optimizers.Adam(learning_rate=args.lr),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# â”€â”€â”€ 5.Â Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ckpt = ModelCheckpoint("best.h5", save_best_only=True, monitor='val_accuracy', mode='max')
history = model.fit(train_ds, validation_data=val_ds,
                    epochs=args.epochs, callbacks=[ckpt])

# â”€â”€â”€ 6.Â Artifacts & quick metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.save("final.h5")
task.upload_artifact("model_final", "final.h5")
task.upload_artifact("model_best",  "best.h5")

y_true = np.concatenate([y.numpy() for _, y in test_ds])
y_pred = np.argmax(model.predict(test_ds), axis=1)
report = classification_report(y_true, y_pred, target_names=class_names)
logger.report_text(report)

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names)
plt.savefig("confusion.png")
task.upload_artifact("confusion_matrix", "confusion.png")

task.close()
print("âœ… quickâ€‘test run complete.")
